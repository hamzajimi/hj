{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Natural_Language_Processing_avec_RNNs_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5cjtsHP8t5Y"
      },
      "source": [
        "#Natural Language Processing\n",
        "Natural Language Processing (NLP) est une discipline informatique qui traite de la communication entre les langues naturelles (humaines) et les langues informatiques. Un exemple courant de NLP est quelque chose comme la vérification orthographique ou l'autocomplétion. La NLP est essentiellement le domaine qui se concentre sur la manière dont les ordinateurs peuvent comprendre et/ou traiter les langues naturelles/humaines. \n",
        "\n",
        "###Réseaux neuronaux récurrents\n",
        "\n",
        "Nous allons présenter un nouveau type de réseau neuronal beaucoup plus capable de traiter des données séquentielles telles que du texte ou des caractères, appelé **réseau neuronal récurrent** (RNN en abrégé). \n",
        "\n",
        "Nous apprendrons à utiliser un réseau neuronal récurrent pour faire ce qui suit :\n",
        "- Analyse des sentiments\n",
        " \n",
        "\n",
        "Les RNN sont complexes et se présentent sous de nombreuses formes différentes. Dans ce TP, nous nous concentrerons donc sur leur fonctionnement et sur le type de problèmes pour lesquels ils sont les mieux adaptés.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gQHK4V4e2wl"
      },
      "source": [
        "##Encodage du texte\n",
        "Comme nous le savons, les modèles d'apprentissage machine et les réseaux de neurones ne prennent pas en compte les données textuelles brutes. Cela signifie que nous devons d'une manière ou d'une autre coder nos données textuelles en valeurs numériques que nos modèles peuvent comprendre. Il existe de nombreuses façons de le faire . \n",
        "\n",
        "Avant d'aborder les  méthodes d'encodage/de prétraitement, voyons quelles informations nous pouvons obtenir à partir de données textuelles en consultant les deux critiques de films suivantes.\n",
        "\n",
        "```Je pensais que le film allait être mauvais, mais il était en fait incroyable !```\n",
        "\n",
        "```Je pensais que le film allait être incroyable, mais il était en fait mauvais !```\n",
        "\n",
        "Bien que ces deux ensembles soient très similaires, nous savons qu'ils ont des significations très différentes. Ceci est dû à l'ordre des mots, une propriété très importante des données textuelles.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZ73YCqqiw9"
      },
      "source": [
        "###Word Embeddings\n",
        "Heureusement, il existe une méthode qui est bien supérieure, **Word Embeddings**. Cette méthode conserve l'ordre des mots intacts et encode les mots similaires avec des étiquettes très similaires. Elle tente de coder non seulement la fréquence et l'ordre des mots, mais aussi la signification de ces mots dans la phrase. Elle encode chaque mot comme un vecteur dense qui représente son contexte dans la phrase.\n",
        "\n",
        "L'incorporation des mots s'apprend en examinant de nombreux exemples de formation différents. Vous pouvez ajouter ce que l'on appelle une *couche d'incorporation* au début de votre modèle et pendant que votre modèle s'entraîne, votre couche d'incorporation apprendra les incorporations correctes des mots. Vous pouvez également utiliser des couches d'incorporation préformées.\n",
        "\n",
        "C'est la technique que nous utiliserons pour nos exemple et sa mise en œuvre sera montrée plus tard.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehig3qliuUzk"
      },
      "source": [
        "##Recurrent Neural Networks (RNN's)\n",
        "Maintenant il est temps de plonger dans les réseaux neuronaux récurrents. Jusqu'à présent, nous avons utilisé ce que l'on appelle des réseaux de neurones à répétition. Cela signifie simplement que toutes nos données sont transmises (d'un seul coup) de gauche à droite à travers le réseau. Cela convenait bien aux problèmes que nous avions envisagés auparavant, mais ne fonctionnera pas très bien pour le traitement de texte. Après tout, même nous (les humains) ne traitons pas le texte en une seule fois. Nous lisons mot par mot de gauche à droite et gardons une trace du sens actuel de la phrase afin de pouvoir comprendre le sens du mot suivant. C'est exactement ce qu'un réseau neuronal récurrent est conçu pour faire. Lorsque nous disons réseau neuronal récurrent, nous voulons simplement dire un réseau qui contient une boucle. Un RNN traitera un mot à la fois tout en conservant une mémoire interne de ce qu'il a déjà vu. Cela lui permettra de traiter les mots différemment en fonction de leur ordre dans une phrase et d'acquérir lentement une compréhension de l'ensemble de l'entrée, un mot à la fois.\n",
        "\n",
        "C'est pourquoi nous traitons nos données textuelles comme une séquence ! Ainsi, nous pouvons transmettre un mot à la fois au RNN.\n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "*Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/*\n",
        "\n",
        "Définissons ce que signifient toutes ces variables avant d'entrer dans l'explication.\n",
        "\n",
        "**h<sub>t</sub>** output at time t\n",
        "\n",
        "**x<sub>t</sub>** input at time t\n",
        "\n",
        "**A** Recurrent Layer (loop)\n",
        "\n",
        "Ce que ce diagramme tente d'illustrer, c'est qu'une couche récurrente traite les mots ou les entrées un à un en combinaison avec les résultats de l'itération précédente. Ainsi, à mesure que nous progressons dans la séquence d'entrée, nous obtenons une compréhension plus complexe du texte dans son ensemble.\n",
        "\n",
        "Ce que nous venons d'examiner s'appelle une **simple couche RNN**. Elle peut être efficace pour traiter des séquences de texte plus courtes pour des problèmes simples, mais elle présente de nombreux inconvénients. L'un d'entre eux est le fait qu'à mesure que les séquences de texte s'allongent, il devient de plus en plus difficile pour le réseau de comprendre correctement le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo3WY-e86zX2"
      },
      "source": [
        "##LSTM\n",
        "La couche dont nous avons discuté en profondeur ci-dessus a été appelée *simpleRNN*. Cependant, il existe d'autres couches récurrentes (couches qui contiennent une boucle) qui fonctionnent beaucoup mieux qu'une simple couche RNN. Celle dont nous allons parler ici est appelée LSTM (Long Short Term Memory). Cette couche fonctionne de manière très similaire à la couche RNN simple mais ajoute un moyen d'accéder aux entrées à partir de n'importe quel pas de temps dans le passé. Alors que dans notre couche RNN simple, les entrées provenant des timestamps précédents disparaissaient progressivement au fur et à mesure que nous avancions dans l'entrée. Avec un LSTM, nous avons une structure de données à mémoire à long terme qui stocke toutes les entrées vues précédemment ainsi que le moment où nous les avons vues. Cela nous permet d'accéder à n'importe quelle valeur antérieure que nous voulons à n'importe quel moment. Cela ajoute à la complexité de notre réseau et lui permet de découvrir des relations plus utiles entre les entrées et le moment où elles apparaissent. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRGOx6_v4eZ_"
      },
      "source": [
        "##Sentiment Analysis\n",
        "Et maintenant, il est temps de voir un réseau neuronal récurrent en action. Pour cet exemple, nous allons faire ce qu'on appelle une analyse des sentiments.\n",
        "\n",
        "Voici la définition formelle de ce terme tirée de Wikipédia :\n",
        "\n",
        "*le processus consistant à identifier et à catégoriser par calcul les opinions exprimées dans un texte, en particulier afin de déterminer si l'attitude de l'auteur à l'égard d'un sujet, d'un produit, etc. est positive, négative ou neutre.*\n",
        "\n",
        "L'exemple que nous utiliserons ici est la classification des revues de films en tant que positives, négatives.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RACGE5Ypt5u9"
      },
      "source": [
        "###Movie Review Dataset\n",
        "Commencez par charger l'ensemble des données de la revue de films IMDB à partir de keras. Cet ensemble de données contient 25 000 revues provenant de la base de données IMDB, chacune d'entre elles étant déjà prétraitée et portant une étiquette positive ou négative. Chaque revue est codée par des nombres entiers qui représentent la fréquence d'un mot dans l'ensemble de la base de données. Par exemple, un mot codé par l'entier 3 signifie qu'il est le troisième mot le plus courant dans l'ensemble de données.\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdsus1kyXWC8",
        "outputId": "3bc1ef32-971b-4af2-ad2c-912b623c67a1"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh6lOpcQ9sIZ",
        "outputId": "95adc858-9011-4bc6-9165-2002d75dc412"
      },
      "source": [
        "# Lets look at one review\n",
        "print(train_data[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     1   194  1153   194  8255    78   228     5     6  1463  4369\n",
            "  5012   134    26     4   715     8   118  1634    14   394    20    13\n",
            "   119   954   189   102     5   207   110  3103    21    14    69   188\n",
            "     8    30    23     7     4   249   126    93     4   114     9  2300\n",
            "  1523     5   647     4   116     9    35  8163     4   229     9   340\n",
            "  1322     4   118     9     4   130  4901    19     4  1002     5    89\n",
            "    29   952    46    37     4   455     9    45    43    38  1543  1905\n",
            "   398     4  1649    26  6853     5   163    11  3215 10156     4  1153\n",
            "     9   194   775     7  8255 11596   349  2637   148   605 15358  8003\n",
            "    15   123   125    68 23141  6853    15   349   165  4362    98     5\n",
            "     4   228     9    43 36893  1157    15   299   120     5   120   174\n",
            "    11   220   175   136    50     9  4373   228  8255     5 25249   656\n",
            "   245  2350     5     4  9837   131   152   491    18 46151    32  7464\n",
            "  1212    14     9     6   371    78    22   625    64  1382     9     8\n",
            "   168   145    23     4  1690    15    16     4  1355     5    28     6\n",
            "    52   154   462    33    89    78   285    16   145    95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAtZHE9-eQ07"
      },
      "source": [
        "###More Preprocessing\n",
        "Si nous regardons certaines de nos revues chargées, nous remarquerons qu'elles sont de longueurs différentes. C'est un problème. Nous ne pouvons pas transmettre des données de longueurs différentes dans notre réseau neuronal. Par conséquent, nous devons faire en sorte que chaque revue ait la même longueur. Pour ce faire, nous suivrons la procédure ci-dessous :\n",
        "- si la revue est supérieure à 250 mots, couper les mots supplémentaires\n",
        "- si la revue est inférieure à 250 mots, ajoutez la quantité nécessaire de 0 pour qu'elle soit égale à 250.\n",
        "\n",
        "Heureusement pour nous, Keras a une fonction qui peut faire cela pour nous :\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3qQ83sNeog6"
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDm_0RTVir7I"
      },
      "source": [
        "###Créer le modèle\n",
        "Il est maintenant temps de créer le modèle. Nous utiliserons une couche d'incorporation de mots comme première couche de notre modèle et nous ajouterons ensuite une couche LSTM qui s'alimentera dans un nœud dense pour obtenir notre sentiment prédit. \n",
        "\n",
        "32 représente la dimension de sortie des vecteurs générés par la couche d'incorporation. Nous pouvons changer cette valeur si nous le souhaitons !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWGGcBIpjrMu"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8_jPL_Kkr-a",
        "outputId": "b1a48049-e021-49e1-ca95-2ce8d2d221f7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyeQCk3LlK6V"
      },
      "source": [
        "###Training\n",
        "Il est maintenant temps de compiler et de former le modèle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKEMjaIulPBe",
        "outputId": "67168480-317b-4f7f-ed1f-9c768b966eb2"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 21s 22ms/step - loss: 0.5307 - acc: 0.7182 - val_loss: 0.3690 - val_acc: 0.8484\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 12s 20ms/step - loss: 0.2352 - acc: 0.9106 - val_loss: 0.3056 - val_acc: 0.8736\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 12s 20ms/step - loss: 0.1791 - acc: 0.9341 - val_loss: 0.3364 - val_acc: 0.8620\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 13s 21ms/step - loss: 0.1427 - acc: 0.9490 - val_loss: 0.2887 - val_acc: 0.8858\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 12s 20ms/step - loss: 0.1227 - acc: 0.9574 - val_loss: 0.3195 - val_acc: 0.8830\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 12s 20ms/step - loss: 0.1037 - acc: 0.9633 - val_loss: 0.3029 - val_acc: 0.8944\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 13s 20ms/step - loss: 0.0867 - acc: 0.9725 - val_loss: 0.3299 - val_acc: 0.8868\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 12s 20ms/step - loss: 0.0751 - acc: 0.9754 - val_loss: 0.4727 - val_acc: 0.8754\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 13s 20ms/step - loss: 0.0697 - acc: 0.9794 - val_loss: 0.3623 - val_acc: 0.8762\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 12s 20ms/step - loss: 0.0632 - acc: 0.9804 - val_loss: 0.3811 - val_acc: 0.8856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3buYlkkhoK93"
      },
      "source": [
        "Et nous évaluerons le modèle sur nos données de training pour voir s'il fonctionne bien."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KImNMWTDoJaQ",
        "outputId": "9372f5a1-8f4b-470e-ccba-404994e61651"
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5036 - acc: 0.8433\n",
            "[0.5035831332206726, 0.8433200120925903]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGrBRC4YCObV"
      },
      "source": [
        "###Faire des prédictions\n",
        "Utilisons maintenant notre réseau pour faire des prévisions sur nos propres examens. \n",
        "\n",
        "Comme nos revues sont bien encodées, nous devons convertir toute revue que nous écrivons sous cette forme pour que le réseau puisse la comprendre. Pour ce faire, chargez bien les encodages de l'ensemble de données et utilisez-les pour encoder nos propres données.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onu8leY4Cn9z",
        "outputId": "6ed77d54-4b72-4afc-bf37-239e01e11b82"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKna3vxmFwrB",
        "outputId": "e5801c62-b1bb-4dcc-ca98-73db9f4aad17"
      },
      "source": [
        "# lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8nyrr00HPZF",
        "outputId": "667583a5-b905-4298-df00-14d0ffe3407a"
      },
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  return (result[0])\n",
        "\n",
        "positive_review = \"that movie was amazing ! i really loved it and it would be great to watch it again because it was amazingly great \"\n",
        "a=predict(positive_review)\n",
        "print('the review is',int(a[0]*100),'% positive')\n",
        "negative_review = \" that movie really sucked.it was really bad. I hated it and wouldn't watch it again.It was one of the worst things I've ever watched\"\n",
        "b=predict(negative_review)\n",
        "print('the review is',100-int(b[0]*100),'% negative')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the review is 83 % positive\n",
            "the review is 94 % negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}